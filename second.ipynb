{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25831086-da2d-4c63-b5c0-dd382d5df636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the dataset directory\n",
    "data_dir = \"./New Plant Diseases Dataset(Augmented)\"\n",
    "\n",
    "# List the contents of the dataset directory\n",
    "print(\"Contents of data_dir:\", os.listdir(data_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804be994-05a2-41d4-9245-ed41c5c195f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                       # for working with files\n",
    "import numpy as np              # for numerical computationss\n",
    "import pandas as pd             # for working with dataframes\n",
    "import torch                    # Pytorch module \n",
    "import matplotlib.pyplot as plt # for plotting informations on graph and images using tensors\n",
    "import torch.nn as nn           # for creating  neural networks\n",
    "from torch.utils.data import DataLoader # for dataloaders \n",
    "from PIL import Image           # for checking images\n",
    "import torch.nn.functional as F # for functions for calculating loss\n",
    "import torchvision.transforms as transforms   # for transforming images into tensors \n",
    "from torchvision.utils import make_grid       # for data checking\n",
    "from torchvision.datasets import ImageFolder  # for working with classes and images\n",
    "from torchsummary import summary              # for getting the summary of our model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72890f47-c62a-4d6e-b6fe-4397691f5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the dataset directory (relative path)\n",
    "data_dir = \"New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "valid_dir = os.path.join(data_dir, \"valid\")\n",
    "\n",
    "# List contents of the directories\n",
    "print(\"Contents of data_dir:\", os.listdir(data_dir))\n",
    "print(\"Contents of train_dir:\", os.listdir(train_dir))\n",
    "print(\"Contents of valid_dir:\", os.listdir(valid_dir))\n",
    "\n",
    "train_dir = data_dir + \"/train\"\n",
    "valid_dir = data_dir + \"/valid\"\n",
    "diseases = os.listdir(train_dir)\n",
    "print(\"Diseases:\", diseases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9f869-42b7-451e-bdce-f5510a90daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plants = []\n",
    "NumberOfDiseases = 0\n",
    "for plant in diseases:\n",
    "    if plant.split('___')[0] not in plants:\n",
    "        plants.append(plant.split('___')[0])\n",
    "    if plant.split('___')[1] != 'healthy':\n",
    "        NumberOfDiseases += 1\n",
    "print(plants)\n",
    "# Save plants array to a file\n",
    "with open('plants.txt', 'w') as file:\n",
    "    for plant in plants:\n",
    "        file.write(plant + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddae9b-b0a0-4214-82e2-5e4aee6cacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images for each disease\n",
    "\n",
    "# Save diseases list to a new text file\n",
    "with open('diseases.txt', 'w') as file:\n",
    "    for disease in diseases:\n",
    "        file.write(disease + '\\n')\n",
    "nums = {}\n",
    "for disease in diseases:\n",
    "    nums[disease] = len(os.listdir(train_dir + '/' + disease))\n",
    "    \n",
    "# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n",
    "\n",
    "img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=[\"no. of images\"])\n",
    "img_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1243b-33a6-4319-8407-f70b9dc59138",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 0\n",
    "for value in nums.values():\n",
    "    n_train += value\n",
    "print(f\"There are {n_train} images for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb7ec0-d028-4b61-8e73-e2e9b014afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets for validation and training\n",
    "train = ImageFolder(train_dir, transform=transforms.ToTensor())\n",
    "valid = ImageFolder(valid_dir, transform=transforms.ToTensor()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0160a6-3c85-4f47-a610-5d2999b8e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# For selecting the default device (GPU if available, else CPU)\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():  # Add parentheses to properly call the function\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# For moving data to the chosen device\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to the chosen device (CPU or GPU)\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "# Wrapper for DataLoader to automatically move batches to the chosen device\n",
    "class DeviceDataLoader:\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d7a10-a6a9-4986-a63e-acf7601aecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For calculating the accuracy\n",
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Calculate accuracy: Compares predictions with actual labels.\n",
    "    Args:\n",
    "        outputs: Model predictions (logits or probabilities).\n",
    "        labels: Actual labels.\n",
    "    Returns:\n",
    "        Accuracy as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    _, preds = torch.max(outputs, dim=1)  # Get predicted class indices\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))  # Compute accuracy\n",
    "\n",
    "\n",
    "# Base class for image classification models\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"\n",
    "        Training step: Computes loss for a batch.\n",
    "        Args:\n",
    "            batch: A tuple containing images and labels.\n",
    "        Returns:\n",
    "            Loss value.\n",
    "        \"\"\"\n",
    "        images, labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate cross-entropy loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        \"\"\"\n",
    "        Validation step: Computes loss and accuracy for a batch.\n",
    "        Args:\n",
    "            batch: A tuple containing images and labels.\n",
    "        Returns:\n",
    "            A dictionary with validation loss and accuracy.\n",
    "        \"\"\"\n",
    "        images, labels = batch\n",
    "        out = self(images)                   # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)          # Calculate accuracy\n",
    "        return {\"val_loss\": loss.detach(), \"val_accuracy\": acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Combine outputs of validation steps across an epoch.\n",
    "        Args:\n",
    "            outputs: List of dictionaries from validation steps.\n",
    "        Returns:\n",
    "            A dictionary with average validation loss and accuracy.\n",
    "        \"\"\"\n",
    "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
    "        batch_accuracy = [x[\"val_accuracy\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Average loss\n",
    "        epoch_accuracy = torch.stack(batch_accuracy).mean()  # Average accuracy\n",
    "        return {\"val_loss\": epoch_loss, \"val_accuracy\": epoch_accuracy}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        \"\"\"\n",
    "        Print epoch metrics.\n",
    "        Args:\n",
    "            epoch: Current epoch number.\n",
    "            result: Dictionary containing epoch metrics (loss, accuracy, etc.).\n",
    "        \"\"\"\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result.get('train_loss', 0), result['val_loss'], result['val_accuracy']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e614ee7-dca0-45da-89f5-c2b77a5b1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Convolution block with BatchNormalization and optional MaxPooling\n",
    "def ConvBlock(in_channels, out_channels, pool=False):\n",
    "    \"\"\"\n",
    "    Creates a convolutional block with BatchNorm and ReLU activation.\n",
    "    Args:\n",
    "        in_channels: Number of input channels.\n",
    "        out_channels: Number of output channels.\n",
    "        pool: Whether to apply MaxPooling.\n",
    "    Returns:\n",
    "        A sequential block with Conv2D, BatchNorm, ReLU, and optional MaxPooling.\n",
    "    \"\"\"\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    ]\n",
    "    if pool:\n",
    "        layers.append(nn.MaxPool2d(4))  # Pooling with kernel size of 4\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# ResNet9 architecture\n",
    "class ResNet9(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes the ResNet9 model.\n",
    "        Args:\n",
    "            in_channels: Number of input channels (e.g., 3 for RGB images).\n",
    "            num_classes: Number of output classes (e.g., number of diseases).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolutional layers\n",
    "        self.conv1 = ConvBlock(in_channels, 64)\n",
    "        self.conv2 = ConvBlock(64, 128, pool=True)  # Downsample with pooling\n",
    "        self.res1 = nn.Sequential(\n",
    "            ConvBlock(128, 128),\n",
    "            ConvBlock(128, 128)\n",
    "        )  # First residual block\n",
    "        \n",
    "        # Deeper convolutional layers\n",
    "        self.conv3 = ConvBlock(128, 256, pool=True)\n",
    "        self.conv4 = ConvBlock(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(\n",
    "            ConvBlock(512, 512),\n",
    "            ConvBlock(512, 512)\n",
    "        )  # Second residual block\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.MaxPool2d(4),  # Global average pooling\n",
    "            nn.Flatten(),     # Flatten for linear layer\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        Args:\n",
    "            xb: Input batch of images.\n",
    "        Returns:\n",
    "            Output logits for each class.\n",
    "        \"\"\"\n",
    "        # Initial convolutional layers\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out  # Residual connection\n",
    "        \n",
    "        # Deeper convolutional layers\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out  # Residual connection\n",
    "        \n",
    "        # Classifier head\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483b9b9-79ee-4285-92d2-9eed7ccec397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = ImageFolder(valid_dir, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b757c6b-d16e-4de4-a0f4-b477b802cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = sorted(os.listdir(valid_dir + '/Apple___Apple_scab')) # since images in test folder are in alphabetical order\n",
    "test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9efcca9-8da3-4afb-9b68-993fd5124182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    \"\"\"Converts image to array and return the predicted class\n",
    "        with highest probability\"\"\"\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "\n",
    "    return train.classes[preds[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8ef66-04d9-4fb1-b8ce-938a284ae065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet9(3, len(train.classes))  # Define the same model architecture\n",
    "model.load_state_dict(torch.load('model.pth', map_location=device))  # Load the model weights onto the device (CPU or GPU)\n",
    "model.to(device)  # Move the model to the appropriate device (GPU or CPU)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "\n",
    "# predicting first image\n",
    "img, label = test[0]\n",
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620b2f2-682f-4c14-91c6-4e1e58d7831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all predictions (actual label vs predicted)\n",
    "for i, (img, label) in enumerate(test):\n",
    "    print('Label:', test_images[i], ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e54d8-79a6-4476-b1ba-8bcdddd23968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b55db-7720-4021-8a79-93fa361f34c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6839ce8-799a-4058-acb7-f5ce50734797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0248689-a7f9-4d4c-9b20-6e3d43e4217a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a8573-82cd-4170-b3c3-cfb3ab974099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c073a-3102-4883-b90c-ca3a14e834a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0bd511-561a-478d-baf0-2a23565732e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd458c-79b9-46c1-b73d-15fc2f6d1f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120fa89-689b-4100-b74d-362a4f398e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4666f7-54e5-4faa-86cf-ee818a841452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323c0d8-a8c7-4101-a9de-86e070117c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de701bf6-7eee-4224-9363-b6c0bd171590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5555c1-f243-4627-9a96-c4e698fdcfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6df94-7509-4087-8d1b-c880b9f34af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884539e3-22e5-45a8-bc8e-a99b533d549b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c336113-bb77-4349-93b8-fdedba1cb586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d923537-363b-4266-a257-db2c7fe7f747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
